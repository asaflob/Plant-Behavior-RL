# -*- coding: utf-8 -*-
"""Value Iteration + Q-Learning for Grid-World

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BVZVpl9kbsHenXCdExJLQ7jdB2gUss_Q

# Value Iteration

### Imports
"""

import numpy as np
import pandas as pd
import time
from IPython.display import display, clear_output

"""### Environment Definition and Setup


"""

ROWS = 10
COLS = 10
STATES = [(row, col) for row in range(ROWS) for col in range(COLS)]

"""
(0, 0), (0, 1), (0, 2),
(1, 0), (1, 1), (1, 2),
(2, 0), (2, 1), (2, 2)
"""

GOAL = (5, 4)
GOAL_REWARD = 100
PITS = {(3, 2), (4,8), (9, 3), (7, 6), (6, 1), (1, 5), (4, 4), (5, 6)}
PIT_REWARD = -50
TERMINAL_STATES = {GOAL}|PITS

STEP_REWARD = -1

# BONUSES = {(5, 5)}
# BONUS_RWARD = 10

ACTIONS = {
    'UP': (-1, 0),
    'DOWN': (1, 0),
    'LEFT': (0, -1),
    'RIGHT': (0, 1)
}

# R(x,x,s') - the immidiate reward for moving into s'=next_state
def get_reward(next_state):
    reward = 0
    if next_state == GOAL:
        reward += GOAL_REWARD
    if next_state in PITS:
        reward += PIT_REWARD
    # if next_state in BONUSES:
    #     reward += BONUS_REWARD
    return reward + STEP_REWARD        # 1 point penalty for all moves


GAMMA = 0.9  # Discount factor
CONVERGENCE_THRESHOLD = 0.1  # When to stop iterating

"""### Helper Functions"""

ACTION_ARROWS = {'UP': '‚Üë', 'DOWN': '‚Üì', 'LEFT': '‚Üê', 'RIGHT': '‚Üí'}  # For drawing the optimal policy grid later on

def get_next_state(state, action):  # Returns (s', R(s,a,s'))
    # If state is terminal it stays terminal
    if state in TERMINAL_STATES:
        return state, 0

    next_state = (state[0]+action[0], state[1]+action[1])
    # If new row or col is out of bounds stay in the same state
    if not (0 <= next_state[0] < ROWS and 0 <= next_state[1] < COLS):
        next_state = state

    return next_state, get_reward(next_state)

def format_table(data, title=""):  # Prints tables nicely using Pandas
    df = pd.DataFrame(data)
    if title:
        print(title)
    display(df.style.format(lambda x: f"{x:,.2f}" if isinstance(x, float) else x))

"""#### initial Board"""

grid = [[get_reward((row, col)) for col in range(COLS)] for row in range(ROWS)]
grid_df = pd.DataFrame(np.full((ROWS, COLS), '', dtype=object))
for state in STATES:
  if state in TERMINAL_STATES:
    grid_df.iloc[state] = f"[{get_reward(state)}]"
  else:
    grid_df.iloc[state] = f"({get_reward(state)})"

format_table(grid_df, "Step-To Rewards Per State\n() = Normal State, [] = Terminal State")

"""### Value Iteration Algorithm"""

def value_iteration(print_all_iterations=False, delay=1):
  # Initialize V(s) = 0 for all states
  V = np.zeros((ROWS, COLS))

  iteration = 0
  while True:
    iteration += 1
    V_new = V.copy()  # New value table for this iteration
    max_delta = 0  # Max change in this iteration

    # Display the current iteration table
    if not print_all_iterations:
      clear_output(wait=True)
    format_table(V, f"--- Iteration {iteration} ---")
    if not print_all_iterations:
      time.sleep(delay) # Pause to make it viewable

    # Loop through all states
    for state in STATES:
      # If it's a terminal state, its value is 0 (no future)
      if state in TERMINAL_STATES:
        V_new[state[0], state[1]] = 0
        continue

      # Calculate the value for each possible action - R(s, a, s') + Œ≥ * V_k(s') for each s'
      action_values = []
      for action in ACTIONS.values():
        next_state, reward = get_next_state(state, action)

        # Bellman update equation: R(s, a, s') + Œ≥ * V_k(s')
        v = reward + GAMMA * V[next_state[0], next_state[1]]
        action_values.append(v)

      # V_k+1(s) = max_a [ R(s, a, s') + Œ≥ * V_k(s') ]
      best_value = max(action_values)
      V_new[state[0], state[1]] = best_value

      # Update the max change this iteration (delta). Used for convergence checking.
      state_delta = abs(V_new[state[0], state[1]] - V[state[0], state[1]])
      max_delta = max(max_delta, state_delta)

    # Finished looping over states
    # Update the value table for the next iteration
    V = V_new

    # Check for convergence
    if max_delta < CONVERGENCE_THRESHOLD:
      if not print_all_iterations:
        clear_output(wait=True)
      print(f"Convergence reached at iteration {iteration}!")
      break

  return V

"""### Policy Extraction"""

def extract_policy(V):
  policy = np.full((ROWS, COLS), '', dtype=object)

  for state in STATES:
    # Handle terminal states
    if state == GOAL:
        policy[state[0], state[1]] = "üëë"
        continue
    if state in PITS:
        policy[state[0], state[1]] = "üí£"
        continue

    # Find the best action
    best_action = None
    best_value = -np.inf

    # Iterate over all possible actions a
    for action_name, action_delta in ACTIONS.items():
        next_state, reward = get_next_state(state, action_delta)  # s', R(s, a, s')

        # v = R(s, a, s') + Œ≥ * V*(s') for CURRENT ACTION a
        v = reward + GAMMA * V[next_state[0], next_state[1]]

        if v > best_value:
            best_value = v
            best_action = ACTION_ARROWS[action_name]

    policy[state[0], state[1]] = best_action

  return policy

"""### Execution"""

if __name__ == "__main__":
    # Note: This check won't run in a Colab cell,
    # but the code will run sequentially.

    # Run Value Iteration
    optimal_V = value_iteration(False, 1)

    # Display the final optimal value table
    format_table(optimal_V, "\n--- Final Optimal Value Table (V*) ---")

    # Extract and display the optimal policy
    optimal_policy = extract_policy(optimal_V)
    format_table(optimal_policy, "\n--- Optimal Policy (œÄ*) ---")

"""# Q-Learning (With Epsilon Decay)

### Imports
"""

import numpy as np
import pandas as pd
import time
from IPython.display import display, clear_output

"""### Environment Definition and Setup


"""

ROWS = 10
COLS = 10
STATES = [(row, col) for row in range(ROWS) for col in range(COLS)]

"""
(0, 0), (0, 1), (0, 2),
(1, 0), (1, 1), (1, 2),
(2, 0), (2, 1), (2, 2)
"""

GOAL = (5, 4)
GOAL_REWARD = 100
PITS = {(3, 2), (4,8), (9, 3), (7, 6), (6, 1), (1, 5), (4, 4), (5, 6)}
PIT_REWARD = -50
TERMINAL_STATES = {GOAL}|PITS

STEP_REWARD = -1

# BONUSES = {(5, 5)}
# BONUS_RWARD = 10

ACTIONS = {
    'UP': (-1, 0),
    'DOWN': (1, 0),
    'LEFT': (0, -1),
    'RIGHT': (0, 1)
}

# R(x,x,s') - the immidiate reward for moving into s'=next_state
def get_reward(next_state):
    reward = 0
    if next_state == GOAL:
        reward = reward + GOAL_REWARD
    if next_state in PITS:
        reward = reward + PIT_REWARD
    # if next_state in BONUSES:
    #     reward = reward + BONUS_REWARD
    return reward + STEP_REWARD        # 1 point penalty for all moves


GAMMA = 0.9  # Discount factor
CONVERGENCE_THRESHOLD = 0.1  # When to stop iterating

"""### Helper Functions"""

ACTION_ARROWS = {'UP': '‚Üë', 'DOWN': '‚Üì', 'LEFT': '‚Üê', 'RIGHT': '‚Üí'}  # For drawing the optimal policy grid later on

def get_next_state(state, action):  # Returns (s', R(s,a,s'))
    # If state is terminal it stays terminal
    if state in TERMINAL_STATES:
        return state, 0

    next_state = (state[0]+action[0], state[1]+action[1])
    # If new row or col is out of bounds stay in the same state
    if not (0 <= next_state[0] < ROWS and 0 <= next_state[1] < COLS):
        next_state = state

    return next_state, get_reward(next_state)


def display_policy_table(policy_matrix, title=""):

    col_names = [f'{c}' for c in range(COLS)]

    df = pd.DataFrame(policy_matrix, index=list(range(ROWS)), columns=col_names)

    if title:
        print(title)
    display(df)

def format_q_table(q_table):
  print("\n--- Q-Learning Final Q-Table (Grid View) ---")
  for action_name, action_vec in ACTIONS.items():
    print(f'\nQ(S, {action_name}):')
    q_grid_for_action = np.zeros((ROWS, COLS))
    for state in STATES:
      q_grid_for_action[state] = q_table[state][action_name]
    df_q_grid = pd.DataFrame(q_grid_for_action, index=list(range(ROWS)), columns=list(range(COLS)))
    display(df_q_grid.style.format("{:,.2f}"))

"""### Q-Learning Hyperparameters"""

ALPHA = 0.1     # Learning Rate (how much we trust new information)
GAMMA = 0.9     # Discount Factor (value of future rewards)
EPSILON_START = 1.0 # Initial exploration rate
EPSILON_END = 0.01  # Minimum exploration rate
N_EPISODES = 20000 # Total episodes for training
DECAY_RATE = 0.0001 # Rate at which epsilon decays per step (approx 10,000 steps to reach END)

# Initialize Q-Table: A dictionary mapping state -> action -> Q-value
# Example: Q_table[(0, 0)] = {'UP': 0.0, 'DOWN': 0.0, 'LEFT': 0.0, 'RIGHT': 0.0}
Q_table = {s: {a: 0.0 for a in ACTIONS} for s in STATES}

"""### Epsilon-Greedy Policy Helper Function"""

def choose_action(state, epsilon):
    # Check if the state is terminal
    if state in TERMINAL_STATES:
        return 'STOP'

    # Decide between Exploration (random) and Exploitation (best-known)
    if np.random.random() < epsilon:
        # EXPLORATION: Choose a random action
        return np.random.choice(list(ACTIONS.keys()))
    else:
        # EXPLOITATION: Choose the action with the highest Q-value
        q_values = Q_table[state]
        # Handle ties by randomly selecting one of the max Q-value actions
        max_q = max(q_values.values())
        best_actions = [a for a, q in q_values.items() if q == max_q]
        return np.random.choice(best_actions)

"""### Q-Learning Algorithm (Training Loop)"""

def run_q_learning():
    epsilon = EPSILON_START

    # Track performance for plotting/monitoring
    rewards_per_episode = []

    print("Starting Q-Learning Training...")

    for episode in range(1, N_EPISODES + 1):
        # Start state (top left corner)
        current_state = (0, 0)
        episode_reward = 0

        while current_state not in TERMINAL_STATES:

            # Epsilon-Greedy Action Selection (Exploitation/Exploration)
            action = choose_action(current_state, epsilon)

            # Environment Interaction (Get S', R = Take step from S to S' and get immidiate reward R)
            next_state, reward = get_next_state(current_state, ACTIONS[action])
            episode_reward += reward

            # Find max_a' Q(s', a')
            if next_state in TERMINAL_STATES:
                max_future_q = 0 # No future from a terminal state
            else:
                max_future_q = max(Q_table[next_state].values())

            #### Q-Learning Update Rule (Bellman Equation) ####

            # Temporal Difference Target (New Info): R + gamma * max_a' Q(s', a')
            td_target = reward + GAMMA * max_future_q

            # Temporal Difference Error: (TD_target - Q(s, a)) = (New Info - Old Info)
            td_error = td_target - Q_table[current_state][action]

            # Update Q-Value: Q(s,a) += alpha * TD_error
            Q_table[current_state][action] += ALPHA * td_error

            ####################################################

            # Transition to next state
            current_state = next_state

            # Epsilon decay
            epsilon = max(EPSILON_END, epsilon - DECAY_RATE)

        rewards_per_episode.append(episode_reward)

        # Display progress every 2000 episodes
        if episode % 2000 == 0:
            clear_output(wait=True)
            print(f"Episode: {episode}/{N_EPISODES}. Epsilon: {epsilon:.4f}. Avg Reward (Last 100): {np.mean(rewards_per_episode[-100:]):.2f}")

    print("Training finished.")
    return Q_table

"""### Policy Extraction"""

def extract_policy(Q_table):
  policy_matrix = np.full((ROWS, COLS), '', dtype=object)

  for state in STATES:
    if state == GOAL:
        policy_matrix[state[0], state[1]] = "üëë"
        continue
    if state in PITS:
        policy_matrix[state[0], state[1]] = "üí£"
        continue

    # Find the action with the maximum Q-value
    q_values = Q_table[state]

    # In case all states are 0 (unvisited states)
    if all(q == 0 for q in q_values.values()):
          policy_matrix[state[0], state[1]] = "‚ùî"
          continue

    max_q = max(q_values.values())
    # Choose the action (a) that yields the best Q(s,a) for this state (s)
    best_action_name = [a for a, q in q_values.items() if q == max_q][0]
    policy_matrix[state[0], state[1]] = ACTION_ARROWS[best_action_name]

  return policy_matrix

"""### Execution"""

if __name__ == "__main__":

    # Run the training
    final_Q_table = run_q_learning()

    # Extract the optimal policy
    optimal_policy = extract_policy(final_Q_table)

    # Display the result
    format_q_table(final_Q_table)
    display_policy_table(optimal_policy, "\n--- Q-Learning Optimal Policy ---")
    print(f"\nüëë = Goal (+{GOAL_REWARD}) | üí£ = Pit (-{PIT_REWARD}) | ‚ùî = Unvisited State")